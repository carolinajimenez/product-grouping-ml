{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-27 20:28:17.551389: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/carolinajimenez/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/carolinajimenez/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/carolinajimenez/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standard library imports.\n",
    "import re\n",
    "import os\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Third party imports.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class DataPreprocessing:\n",
    "    \"\"\"\n",
    "    This class handles preprocessing tasks for the dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the DataPreprocessing object.\n",
    "        \"\"\"\n",
    "        self.items_df = None\n",
    "        self.item_pictures_df = None\n",
    "        self.dataset_path = \"../data/raw\"\n",
    "\n",
    "    def load_dataframes(self, items_path, item_pictures_path):\n",
    "        \"\"\"\n",
    "        Load the datasets into the class.\n",
    "\n",
    "        Parameters:\n",
    "        items_path (str): File path of the items dataset.\n",
    "        item_pictures_path (str): File path of the item pictures dataset.\n",
    "        \"\"\"\n",
    "        self.items_df = pd.read_csv(f\"{self.dataset_path}/{items_path}.csv\")\n",
    "        self.item_pictures_df = pd.read_csv(f\"{self.dataset_path}/{item_pictures_path}.csv\")\n",
    "\n",
    "    def remove_columns(self, dataframe, columns_to_remove):\n",
    "        \"\"\"\n",
    "        Remove specified columns from the dataframe.\n",
    "\n",
    "        Parameters:\n",
    "        dataframe (pandas.DataFrame): The dataframe from which columns need to be removed.\n",
    "        columns_to_remove (list): List of column names to be removed.\n",
    "        \"\"\"\n",
    "        dataframe.drop(columns=columns_to_remove, inplace=True)\n",
    "\n",
    "    def handle_missing_values(self, dataframe):\n",
    "        \"\"\"\n",
    "        Replace missing values in the dataframe with an empty string.\n",
    "\n",
    "        Parameters:\n",
    "        dataframe (pandas.DataFrame): The dataframe in which missing values need to be handled.\n",
    "        \"\"\"\n",
    "        dataframe.fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = DataPreprocessing()\n",
    "\n",
    "# Load dataframes\n",
    "preprocessor.load_dataframes(\"items_MCO\", \"item_pictures_MCO\")\n",
    "\n",
    "# Remove specified columns\n",
    "columns_to_remove = ['thumbnail_id', 'catalog_product_id',\n",
    "       'permalink', 'site_id', 'category_id', 'currency_id',\n",
    "       'order_backend', 'price', 'original_price', 'sale_price',\n",
    "       'available_quantity', 'official_store_id', 'use_thumbnail_id',\n",
    "       'accepts_mercadopago', 'stop_time', 'winner_item_id', 'catalog_listing',\n",
    "       'discounts', 'promotions', 'inventory_id', 'store_pick_up',\n",
    "       'free_shipping', 'logistic_type', 'mode', 'tags', 'benefits',\n",
    "       'promise', 'quantity', 'amount', 'rate', 'seller_id','initial_quantity',\n",
    "       'warranty', 'differential_pricing', 'variation_filters', 'variations_data',\n",
    "       'official_store_name', 'location', 'seller_contact']\n",
    "preprocessor.remove_columns(preprocessor.items_df, columns_to_remove)\n",
    "\n",
    "columns_to_remove = ['site_id', 'category_id', 'secure_url', 'size',\n",
    "       'max_size', 'quality']\n",
    "preprocessor.remove_columns(preprocessor.item_pictures_df, columns_to_remove)\n",
    "\n",
    "# Handle missing values\n",
    "preprocessor.handle_missing_values(preprocessor.items_df)\n",
    "preprocessor.handle_missing_values(preprocessor.item_pictures_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"../data/processed\"\n",
    "def convert_and_save_dataframe(arr, df_name):\n",
    "    dataframe = pd.DataFrame(arr)\n",
    "    dataframe.to_csv(f\"{dataset_path}/{df_name}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_and_save_dataframe(preprocessor.items_df, \"items\")\n",
    "convert_and_save_dataframe(preprocessor.item_pictures_df, \"item_pictures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.vectorizer = TfidfVectorizer(stop_words='english')\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"\n",
    "        Preprocesses text data.\n",
    "\n",
    "        Args:\n",
    "        text (str): Input text to preprocess.\n",
    "\n",
    "        Returns:\n",
    "        str: Preprocessed text.\n",
    "        \"\"\"\n",
    "        # Convert text to lowercase\n",
    "        text = text.lower()\n",
    "        # Remove special characters, numbers, and punctuation\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        return text\n",
    "\n",
    "    def preprocess_features(self, items_df, pictures_df):\n",
    "        \"\"\"\n",
    "        Preprocesses features including text and images.\n",
    "\n",
    "        Args:\n",
    "        items_df (DataFrame): DataFrame containing item data.\n",
    "        pictures_df (DataFrame): DataFrame containing picture data.\n",
    "\n",
    "        Returns:\n",
    "        DataFrame: Preprocessed feature DataFrame.\n",
    "        \"\"\"\n",
    "        # Merge items and pictures dataframes on item_id\n",
    "        merged_df = pd.merge(items_df, pictures_df.groupby('item_id').first(), on='item_id', how='left')\n",
    "\n",
    "        # Preprocess text features\n",
    "        merged_df['title'] = merged_df['title'].apply(self.preprocess_text)\n",
    "        merged_df['official_store_name'] = merged_df['official_store_name'].apply(self.preprocess_text)\n",
    "\n",
    "        # Normalize numerical features\n",
    "        numerical_features = ['price', 'warranty']\n",
    "        merged_df[numerical_features] = self.scaler.fit_transform(merged_df[numerical_features])\n",
    "\n",
    "        # Select relevant features\n",
    "        selected_features = ['title', 'condition_new', 'condition_not_specified', 'condition_used',\n",
    "                             'listing_type_id_bronze', 'listing_type_id_free', 'listing_type_id_gold',\n",
    "                             'listing_type_id_gold_premium', 'listing_type_id_gold_pro', 'listing_type_id_gold_special',\n",
    "                             'listing_type_id_silver', 'buying_mode_buy_it_now', 'buying_mode_classified',\n",
    "                             'thumbnail', 'price', 'seller_id', 'warranty', 'official_store_name', 'url']\n",
    "\n",
    "        preprocessed_df = merged_df[selected_features]\n",
    "        \n",
    "        return preprocessed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'item_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/jd/bf1pfxf55sschy05hfjrlklr0000gn/T/ipykernel_2903/1253441676.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Instantiate DataPreprocessor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mpreprocessor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataPreprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Preprocess features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mpreprocessed_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpictures_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/jd/bf1pfxf55sschy05hfjrlklr0000gn/T/ipykernel_2903/3030764123.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, items_df, pictures_df)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mReturns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mDataFrame\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPreprocessed\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \"\"\"\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m# Merge items and pictures dataframes on item_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mmerged_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpictures_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'item_id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'item_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# Preprocess text features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mmerged_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerged_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/DEVS/aidevs/mlenv/lib/python3.11/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mvalidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         )\n\u001b[1;32m    168\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m         op = _MergeOperation(\n\u001b[0m\u001b[1;32m    170\u001b[0m             \u001b[0mleft_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0mright_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/DEVS/aidevs/mlenv/lib/python3.11/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[1;32m    787\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright_join_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m             \u001b[0mleft_drop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[0mright_drop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 791\u001b[0;31m         ) = self._get_merge_keys()\n\u001b[0m\u001b[1;32m    792\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mleft_drop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_labels_or_levels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft_drop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/DEVS/aidevs/mlenv/lib/python3.11/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1283\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mlk\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m                         \u001b[0;31m# Then we're either Hashable or a wrong-length arraylike,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m                         \u001b[0;31m#  the latter of which will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1286\u001b[0m                         \u001b[0mlk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHashable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1287\u001b[0;31m                         \u001b[0mleft_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1288\u001b[0m                         \u001b[0mjoin_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1289\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m                         \u001b[0;31m# work-around for merge_asof(left_index=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/DEVS/aidevs/mlenv/lib/python3.11/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1840\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mother_axes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1841\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_level_reference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1842\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1843\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1844\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m         \u001b[0;31m# Check for duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'item_id'"
     ]
    }
   ],
   "source": [
    "dataset_path = \"../data/processed\"\n",
    "items_file = f\"{dataset_path}/items.csv\"\n",
    "pictures_file = f\"{dataset_path}/item_pictures.csv\"\n",
    "items_df = pd.read_csv(items_file)\n",
    "pictures_df = pd.read_csv(pictures_file)\n",
    "\n",
    "# Instantiate DataPreprocessor\n",
    "preprocessor = DataPreprocessor()\n",
    "\n",
    "# Preprocess features\n",
    "preprocessed_data = preprocessor.preprocess_features(items_df, pictures_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocessor:\n",
    "     \"\"\"\n",
    "     Class for preprocessing data for machine learning models.\n",
    "     \"\"\"\n",
    "\n",
    "     def __init__(self, items_file, pictures_file):\n",
    "          \"\"\"\n",
    "          Initialize the DataPreprocessor object.\n",
    "\n",
    "          Parameters:\n",
    "          - items_file (str): Path to the CSV file containing item data.\n",
    "          - pictures_file (str): Path to the CSV file containing picture data.\n",
    "          \"\"\"\n",
    "          self.items_df = pd.read_csv(items_file)\n",
    "          self.pictures_df = pd.read_csv(pictures_file)\n",
    "          self.lemmatizer = WordNetLemmatizer()\n",
    "          self.vectorizer = CountVectorizer(lowercase=True, stop_words='english', max_features=100)\n",
    "          self.image_model = VGG16(weights='imagenet', include_top=False)\n",
    "          self.pca = PCA(n_components=100)  # Reduce image features to 100 dimensions\n",
    "\n",
    "     def preprocess_text(self, text):\n",
    "          \"\"\"\n",
    "          Preprocess text data by tokenization and lemmatization.\n",
    "\n",
    "          Parameters:\n",
    "          - text (str): Input text to be preprocessed.\n",
    "\n",
    "          Returns:\n",
    "          - str: Preprocessed text.\n",
    "          \"\"\"\n",
    "          tokens = word_tokenize(text)\n",
    "          lemmatized_tokens = [self.lemmatizer.lemmatize(token) for token in tokens]\n",
    "          return ' '.join(lemmatized_tokens)\n",
    "\n",
    "     def preprocess_image_features(self, item_id):\n",
    "          \"\"\"\n",
    "          Preprocess image features for a given item ID.\n",
    "\n",
    "          Parameters:\n",
    "          - item_id (str): ID of the item to preprocess images for.\n",
    "\n",
    "          Returns:\n",
    "          - numpy.ndarray: Array of preprocessed image features.\n",
    "          \"\"\"\n",
    "          thumbnail_url = self.items_df[self.items_df['id'] == item_id]['thumbnail'].values[0]\n",
    "          image_urls = self.pictures_df[self.pictures_df['item_id'] == item_id]['url'].tolist()\n",
    "          image_urls = [str(thumbnail_url)] + [str(url) for url in image_urls[:1]]  # Convert URLs to string\n",
    "          image_features = []\n",
    "          for url in image_urls:\n",
    "               if os.path.exists(url):\n",
    "                    img = image.load_img(url, target_size=(224, 224))\n",
    "                    img_array = image.img_to_array(img)\n",
    "                    img_array = np.expand_dims(img_array, axis=0)\n",
    "                    img_array = preprocess_input(img_array)\n",
    "                    features = self.image_model.predict(img_array)\n",
    "                    flattened_features = features.flatten()\n",
    "                    image_features.append(flattened_features)\n",
    "          if image_features:\n",
    "               image_features = np.array(image_features)\n",
    "               image_features = self.pca.fit_transform(image_features)\n",
    "          else:\n",
    "               image_features = np.zeros((1, 100))\n",
    "          return image_features\n",
    "\n",
    "\n",
    "     def preprocess_features(self):\n",
    "          \"\"\"\n",
    "          Preprocess features for machine learning models.\n",
    "\n",
    "          Returns:\n",
    "          - pandas.DataFrame: Preprocessed features.\n",
    "          \"\"\"\n",
    "          # Preprocess text features\n",
    "          self.items_df['title'] = self.items_df['title'].apply(self.preprocess_text)\n",
    "          text_features = self.vectorizer.fit_transform(self.items_df['title']).toarray()\n",
    "          text_feature_names = self.vectorizer.get_feature_names_out()\n",
    "          text_df = pd.DataFrame(text_features, columns=text_feature_names)\n",
    "\n",
    "          # Preprocess image features\n",
    "          image_features = []\n",
    "          for item_id in self.items_df['id']:\n",
    "               item_features = self.preprocess_image_features(item_id)\n",
    "               image_features.append(item_features)\n",
    "          image_features = np.vstack(image_features)\n",
    "          image_feature_names = [f\"image_feature_{i}\" for i in range(image_features.shape[1])]\n",
    "          image_df = pd.DataFrame(image_features, columns=image_feature_names)\n",
    "\n",
    "          # Combine text and image features\n",
    "          all_features = pd.concat([text_df, image_df], axis=1)\n",
    "\n",
    "          # Select relevant features\n",
    "          selected_features = ['price', 'warranty']\n",
    "\n",
    "          all_features = pd.concat([all_features, self.items_df[selected_features]], axis=1)\n",
    "\n",
    "          return all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate DataPreprocessor\n",
    "dataset_path = \"../data/processed\"\n",
    "items_file = f\"{dataset_path}/items.csv\"\n",
    "pictures_file = f\"{dataset_path}/item_pictures.csv\"\n",
    "preprocessor = DataPreprocessor(items_file, pictures_file)\n",
    "\n",
    "# Preprocess features\n",
    "features = preprocessor.preprocess_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "202"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = list(features.columns)\n",
    "len(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"../data/processed\"\n",
    "def convert_and_save_dataframe(arr, df_name):\n",
    "    dataframe = pd.DataFrame(arr)\n",
    "    dataframe.to_csv(f\"{dataset_path}/{df_name}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_and_save_dataframe(features, \"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor:\n",
    "    \"\"\"\n",
    "    Class for feature extraction from product titles and images.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.image_model = VGG16(weights='imagenet', include_top=False)\n",
    "        self.pca = PCA(n_components=100)  # Reduce image features to 100 dimensions\n",
    "\n",
    "    def extract_text_features(self, titles):\n",
    "        \"\"\"\n",
    "        Extract features from product titles using NLP techniques.\n",
    "\n",
    "        Parameters:\n",
    "        - titles (list): List of product titles.\n",
    "\n",
    "        Returns:\n",
    "        - numpy.ndarray: Array of text features.\n",
    "        \"\"\"\n",
    "        text_features = []\n",
    "        for title in titles:\n",
    "            tokens = word_tokenize(title)\n",
    "            # Lemmatize tokens\n",
    "            lemmatized_tokens = [self.lemmatizer.lemmatize(token) for token in tokens]\n",
    "            # Convert tokens to lowercase\n",
    "            normalized_tokens = [token.lower() for token in lemmatized_tokens]\n",
    "            text_features.append(normalized_tokens)\n",
    "        return np.array(text_features)\n",
    "\n",
    "    def extract_image_features(self, image_paths):\n",
    "        \"\"\"\n",
    "        Extract features from product images using pre-trained CNNs.\n",
    "\n",
    "        Parameters:\n",
    "        - image_paths (list): List of paths to product images.\n",
    "\n",
    "        Returns:\n",
    "        - numpy.ndarray: Array of image features.\n",
    "        \"\"\"\n",
    "        image_features = []\n",
    "        for path in image_paths:\n",
    "            if os.path.exists(path):  # Check if image file exists\n",
    "                try:\n",
    "                    img = image.load_img(path, target_size=(224, 224))\n",
    "                    img_array = image.img_to_array(img)\n",
    "                    img_array = np.expand_dims(img_array, axis=0)\n",
    "                    img_array = preprocess_input(img_array)\n",
    "                    features = self.image_model.predict(img_array)\n",
    "                    flattened_features = features.flatten()\n",
    "                    image_features.append(flattened_features)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing image {path}: {e}\")\n",
    "            else:\n",
    "                print(f\"Image file not found: {path}\")\n",
    "        if image_features:\n",
    "            image_features = np.array(image_features)\n",
    "            image_features = self.pca.fit_transform(image_features)  # Reduce dimensionality\n",
    "        else:\n",
    "            image_features = np.zeros((len(image_paths), 100))  # Return zeros if no valid images\n",
    "        return image_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'title'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/DEVS/aidevs/mlenv/lib/python3.11/site-packages/pandas/core/indexes/base.py:3790\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3789\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3790\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3791\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'title'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m text_features \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m features\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m----> 4\u001b[0m     title \u001b[38;5;241m=\u001b[39m \u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtitle\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# condition_features = row[['condition_new', 'condition_not_specified', 'condition_used']].tolist()\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# listing_type_features = row[[col for col in features.columns if 'listing_type_id' in col]].tolist()\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# buying_mode_features = row[[col for col in features.columns if 'buying_mode' in col]].tolist()\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# seller_id = row['seller_id']\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     warranty \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwarranty\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/DEVS/aidevs/mlenv/lib/python3.11/site-packages/pandas/core/series.py:1040\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m-> 1040\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[1;32m   1044\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[0;32m~/Documents/DEVS/aidevs/mlenv/lib/python3.11/site-packages/pandas/core/series.py:1156\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1153\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1156\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[1;32m   1159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[0;32m~/Documents/DEVS/aidevs/mlenv/lib/python3.11/site-packages/pandas/core/indexes/base.py:3797\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3792\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3793\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3794\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3795\u001b[0m     ):\n\u001b[1;32m   3796\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3797\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3798\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3799\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3800\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3801\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3802\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'title'"
     ]
    }
   ],
   "source": [
    "# Extract text features\n",
    "text_features = []\n",
    "for _, row in features.iterrows():\n",
    "    title = row['title']\n",
    "    # condition_features = row[['condition_new', 'condition_not_specified', 'condition_used']].tolist()\n",
    "    # listing_type_features = row[[col for col in features.columns if 'listing_type_id' in col]].tolist()\n",
    "    # buying_mode_features = row[[col for col in features.columns if 'buying_mode' in col]].tolist()\n",
    "    # seller_id = row['seller_id']\n",
    "    warranty = row['warranty']\n",
    "    # official_store_name = row['official_store_name']\n",
    "    \n",
    "    # Combine all text features\n",
    "    # combined_text = f\"{title} {seller_id} {warranty} {official_store_name}\"\n",
    "    combined_text = f\"{title} {warranty}\"\n",
    "    # combined_text += \" \".join(map(str, condition_features))\n",
    "    # combined_text += \" \".join(map(str, listing_type_features))\n",
    "    # combined_text += \" \".join(map(str, buying_mode_features))\n",
    "    text_features.append(combined_text)\n",
    "\n",
    "# Extract image features\n",
    "image_paths = features['images'].explode().tolist()\n",
    "extractor = FeatureExtractor()\n",
    "image_features = extractor.extract_image_features(image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtext_features\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text_features' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"text_features:\", text_features[0])\n",
    "print(\"image_features:\", image_features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
