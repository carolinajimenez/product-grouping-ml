{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-27 21:39:55.276068: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/carolinajimenez/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/carolinajimenez/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/carolinajimenez/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standard library imports.\n",
    "import re\n",
    "import os\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Third party imports.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class DataPreprocessing:\n",
    "    \"\"\"\n",
    "    This class handles preprocessing tasks for the dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the DataPreprocessing object.\n",
    "        \"\"\"\n",
    "        self.items_df = None\n",
    "        self.item_pictures_df = None\n",
    "        self.dataset_path = \"../data/raw\"\n",
    "\n",
    "    def load_dataframes(self, items_path, item_pictures_path):\n",
    "        \"\"\"\n",
    "        Load the datasets into the class.\n",
    "\n",
    "        Parameters:\n",
    "        items_path (str): File path of the items dataset.\n",
    "        item_pictures_path (str): File path of the item pictures dataset.\n",
    "        \"\"\"\n",
    "        self.items_df = pd.read_csv(f\"{self.dataset_path}/{items_path}.csv\")\n",
    "        self.item_pictures_df = pd.read_csv(f\"{self.dataset_path}/{item_pictures_path}.csv\")\n",
    "\n",
    "    def remove_columns(self, dataframe, columns_to_remove):\n",
    "        \"\"\"\n",
    "        Remove specified columns from the dataframe.\n",
    "\n",
    "        Parameters:\n",
    "        dataframe (pandas.DataFrame): The dataframe from which columns need to be removed.\n",
    "        columns_to_remove (list): List of column names to be removed.\n",
    "        \"\"\"\n",
    "        dataframe.drop(columns=columns_to_remove, inplace=True)\n",
    "\n",
    "    def handle_missing_values(self, dataframe):\n",
    "        \"\"\"\n",
    "        Replace missing values in the dataframe with an empty string.\n",
    "\n",
    "        Parameters:\n",
    "        dataframe (pandas.DataFrame): The dataframe in which missing values need to be handled.\n",
    "        \"\"\"\n",
    "        dataframe.fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = DataPreprocessing()\n",
    "\n",
    "# Load dataframes\n",
    "preprocessor.load_dataframes(\"items_MCO\", \"item_pictures_MCO\")\n",
    "\n",
    "# Remove specified columns\n",
    "columns_to_remove = ['thumbnail_id', 'catalog_product_id',\n",
    "       'permalink', 'site_id', 'category_id', 'currency_id',\n",
    "       'order_backend', 'price', 'original_price', 'sale_price',\n",
    "       'available_quantity', 'official_store_id', 'use_thumbnail_id',\n",
    "       'accepts_mercadopago', 'stop_time', 'winner_item_id', 'catalog_listing',\n",
    "       'discounts', 'promotions', 'inventory_id', 'store_pick_up',\n",
    "       'free_shipping', 'logistic_type', 'mode', 'tags', 'benefits',\n",
    "       'promise', 'quantity', 'amount', 'rate', 'seller_id','initial_quantity',\n",
    "       'warranty', 'differential_pricing', 'variation_filters', 'variations_data',\n",
    "       'official_store_name', 'location', 'seller_contact']\n",
    "preprocessor.remove_columns(preprocessor.items_df, columns_to_remove)\n",
    "\n",
    "columns_to_remove = ['site_id', 'category_id', 'secure_url', 'size',\n",
    "       'max_size', 'quality']\n",
    "preprocessor.remove_columns(preprocessor.item_pictures_df, columns_to_remove)\n",
    "\n",
    "# Handle missing values\n",
    "preprocessor.handle_missing_values(preprocessor.items_df)\n",
    "preprocessor.handle_missing_values(preprocessor.item_pictures_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"../data/processed\"\n",
    "def convert_and_save_dataframe(arr, df_name):\n",
    "    dataframe = pd.DataFrame(arr)\n",
    "    dataframe.to_csv(f\"{dataset_path}/{df_name}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_and_save_dataframe(preprocessor.items_df, \"items\")\n",
    "convert_and_save_dataframe(preprocessor.item_pictures_df, \"item_pictures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
