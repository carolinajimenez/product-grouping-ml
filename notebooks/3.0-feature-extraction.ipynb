{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports.\n",
    "import re\n",
    "\n",
    "# Third party imports.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FeatureExtraction:\n",
    "    \"\"\"\n",
    "    This class handles feature extraction tasks for the dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the FeatureExtraction object.\n",
    "        \"\"\"\n",
    "        self.items_df = None\n",
    "        self.item_pictures_df = None\n",
    "        self.dataset_path = \"../data/processed\"\n",
    "\n",
    "        self.nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "    def load_dataframes(self, items_path, item_pictures_path):\n",
    "        \"\"\"\n",
    "        Load the datasets into the class.\n",
    "\n",
    "        Parameters:\n",
    "        items_path (str): File path of the items dataset.\n",
    "        item_pictures_path (str): File path of the item pictures dataset.\n",
    "        \"\"\"\n",
    "        self.items_df = pd.read_csv(f\"{self.dataset_path}/{items_path}.csv\")\n",
    "        self.item_pictures_df = pd.read_csv(f\"{self.dataset_path}/{item_pictures_path}.csv\")\n",
    "\n",
    "    def concatenate_text_variables(self):\n",
    "        \"\"\"\n",
    "        Concatenate selected text variables into a single column.\n",
    "        \"\"\"\n",
    "        text_columns = ['title', 'condition', 'listing_type_id', 'buying_mode', 'domain_id']\n",
    "        self.items_df['text_features'] = self.items_df[text_columns].fillna('').apply(lambda x: ' '.join(x), axis=1)\n",
    "\n",
    "    def make_text_machine_friendly(self, text):\n",
    "        \"\"\"\n",
    "        Apply text preprocessing techniques to make text machine-friendly.\n",
    "\n",
    "        Parameters:\n",
    "        text (str): Input text.\n",
    "\n",
    "        Returns:\n",
    "        str: Preprocessed text.\n",
    "        \"\"\"\n",
    "        # Convert words into lowercase\n",
    "        text = text.lower()\n",
    "\n",
    "        # Remove leading and trailing whitespaces\n",
    "        text = text.strip()\n",
    "\n",
    "        # Remove punctuation\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "        # Remove stopwords\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        word_tokens = word_tokenize(text)\n",
    "        filtered_text = [word for word in word_tokens if word not in stop_words]\n",
    "        text = ' '.join(filtered_text)\n",
    "\n",
    "        # Expand contractions (if needed)\n",
    "\n",
    "        # Remove special characters (numbers, emojis, etc.)\n",
    "        text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "\n",
    "        return text\n",
    "\n",
    "    def apply_tokenization_and_lemmatization(self, text):\n",
    "        \"\"\"\n",
    "        Apply tokenization and lemmatization to the text.\n",
    "\n",
    "        Parameters:\n",
    "        text (str): Input text.\n",
    "\n",
    "        Returns:\n",
    "        list: List of lemmatized tokens.\n",
    "        \"\"\"\n",
    "        doc = self.nlp(text)\n",
    "        lemmatized_tokens = [token.lemma_ for token in doc]\n",
    "        return lemmatized_tokens\n",
    "\n",
    "    def get_text_features(self):\n",
    "        \"\"\"\n",
    "        Obtain the feature vector for text.\n",
    "\n",
    "        Returns:\n",
    "        numpy.ndarray: Feature vector for text.\n",
    "        \"\"\"\n",
    "        text_features = self.items_df['text_features'].apply(self.make_text_machine_friendly)\n",
    "        text_features = text_features.apply(self.apply_tokenization_and_lemmatization)\n",
    "        text_features = text_features.apply(lambda x: ' '.join(x))\n",
    "        return text_features\n",
    "\n",
    "    def get_image_features(self, model_name='VGG16'):\n",
    "        \"\"\"\n",
    "        Obtain the feature vector for images using pre-trained CNN models.\n",
    "\n",
    "        Parameters:\n",
    "        model_name (str): Name of the pre-trained CNN model.\n",
    "\n",
    "        Returns:\n",
    "        numpy.ndarray: Feature vector for images.\n",
    "        \"\"\"\n",
    "        if model_name == 'VGG16':\n",
    "            model = VGG16(weights='imagenet', include_top=False, pooling='avg')\n",
    "            feature_list = []\n",
    "\n",
    "            # Create a dictionary to store URLs for each item_id\n",
    "            img_data = {}\n",
    "            for _, row in self.item_pictures_df.iterrows():\n",
    "                item_id = row['item_id']\n",
    "                url = row['url']\n",
    "                if item_id not in img_data:\n",
    "                    img_data[item_id] = {'url': '', 'thumbnail': ''}\n",
    "                # Only take the first URL for each item_id\n",
    "                if not img_data[item_id]['url']:\n",
    "                    img_data[item_id]['url'] = url\n",
    "\n",
    "            for _, row in tqdm(self.items_df.iterrows(), total=len(self.items_df)):\n",
    "                item_id = row['id']\n",
    "                thumbnail_url = row['thumbnail']\n",
    "                if item_id not in img_data:\n",
    "                    continue  # Skip if item_id not found in img_data (no associated URL found)\n",
    "\n",
    "                img_thumbnail = image.load_img(thumbnail_url, target_size=(224, 224))\n",
    "                img_array_thumbnail = image.img_to_array(img_thumbnail)\n",
    "                img_array_thumbnail = np.expand_dims(img_array_thumbnail, axis=0)\n",
    "                img_array_thumbnail = preprocess_input(img_array_thumbnail)\n",
    "\n",
    "                features_thumbnail = model.predict(img_array_thumbnail)\n",
    "                feature_list.append(features_thumbnail.flatten())\n",
    "\n",
    "                # Process URL images\n",
    "                url = img_data[item_id]['url']\n",
    "                if url:\n",
    "                    img_url = image.load_img(url, target_size=(224, 224))\n",
    "                    img_array_url = image.img_to_array(img_url)\n",
    "                    img_array_url = np.expand_dims(img_array_url, axis=0)\n",
    "                    img_array_url = preprocess_input(img_array_url)\n",
    "\n",
    "                    features_url = model.predict(img_array_url)\n",
    "                    feature_list.append(features_url.flatten())\n",
    "\n",
    "            image_features = np.array(feature_list)\n",
    "            return image_features\n",
    "        else:\n",
    "            raise ValueError(\"Invalid model name. Please choose from 'VGG16'.\")\n",
    "\n",
    "    def save_features(self, text_features, image_features, text_output_path, image_output_path):\n",
    "        \"\"\"\n",
    "        Save the extracted features to files.\n",
    "\n",
    "        Parameters:\n",
    "        text_features (numpy.ndarray): Feature vector for text.\n",
    "        image_features (numpy.ndarray): Feature vector for images.\n",
    "        text_output_path (str): File path to save text features.\n",
    "        image_output_path (str): File path to save image features.\n",
    "        \"\"\"\n",
    "        np.save(f\"{self.dataset_path}/{text_output_path}\", text_features)\n",
    "        np.save(f\"{self.dataset_path}/{image_output_path}\", image_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fe = FeatureExtraction()\n",
    "\n",
    "# Load dataframes\n",
    "fe.load_dataframes(\"items\", \"item_pictures\")\n",
    "\n",
    "# Concatenate text variables\n",
    "fe.concatenate_text_variables()\n",
    "\n",
    "# Obtain text features\n",
    "text_features = fe.get_text_features()\n",
    "\n",
    "# Obtain image features\n",
    "image_features = fe.get_image_features(\"image_directory_path\")\n",
    "\n",
    "# Save features\n",
    "fe.save_features(text_features, image_features, \"text_features.npy\", \"image_features.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"text_features:\", type(text_features))\n",
    "print(\"image_features:\", type(image_features))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
